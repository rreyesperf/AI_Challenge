"""
API integration tests for local LLM endpoints
Tests the Flask API endpoints with local LLM providers
"""

import unittest
import sys
import os
import json
from unittest.mock import Mock, patch
import requests

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from app import app
    from services.llm_service import llm_service
    FLASK_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Could not import Flask app: {e}")
    FLASK_AVAILABLE = False
    app = None


class TestLocalLLMAPI(unittest.TestCase):
    """Test API endpoints with local LLM integration"""
    
    def setUp(self):
        """Set up test client"""
        if FLASK_AVAILABLE and app:
            self.app = app
            self.app.config['TESTING'] = True
            self.client = self.app.test_client()
            self.api_base_url = "http://localhost:5000"
        else:
            self.skipTest("Flask app not available")
    
    def test_ai_providers_endpoint(self):
        """Test the AI providers endpoint"""
        response = self.client.get('/api/ai/providers')
        self.assertEqual(response.status_code, 200)
        
        data = json.loads(response.data)
        self.assertIn('available_providers', data)
        self.assertIn('total_count', data)
        self.assertIsInstance(data['available_providers'], list)
        self.assertIsInstance(data['total_count'], int)
    
    def test_ai_health_endpoint(self):
        """Test the AI health endpoint"""
        response = self.client.get('/api/ai/health')
        self.assertEqual(response.status_code, 200)
        
        data = json.loads(response.data)
        self.assertIn('status', data)
        self.assertIn('available_providers', data)
        self.assertIn('providers', data)
        self.assertIn('timestamp', data)
    
    @patch('routes.llm_service')
    def test_ai_chat_with_local_provider(self, mock_llm_service):
        """Test AI chat endpoint with local LLM provider"""
        # Mock successful response
        mock_llm_service.generate_response.return_value = {
            "success": True,
            "response": "This is a test response from local LLM",
            "provider": "ollama",
            "model": "llama2"
        }
        
        payload = {
            "message": "Test message for local LLM",
            "provider": "ollama",
            "max_tokens": 100,
            "temperature": 0.7
        }
        
        response = self.client.post('/api/ai/chat',
                                  data=json.dumps(payload),
                                  content_type='application/json')
        
        self.assertEqual(response.status_code, 200)
        data = json.loads(response.data)
        self.assertTrue(data['success'])
        self.assertEqual(data['provider'], 'ollama')
    
    @patch('routes.llm_service')
    def test_ai_chat_error_handling(self, mock_llm_service):
        """Test AI chat endpoint error handling"""
        # Mock error response
        mock_llm_service.generate_response.return_value = {
            "success": False,
            "error": "Local LLM server not available",
            "provider": "ollama",
            "model": "llama2"
        }
        
        payload = {
            "message": "Test message",
            "provider": "ollama"
        }
        
        response = self.client.post('/api/ai/chat',
                                  data=json.dumps(payload),
                                  content_type='application/json')
        
        self.assertEqual(response.status_code, 200)  # API returns 200 with error in body
        data = json.loads(response.data)
        self.assertFalse(data['success'])
        self.assertIn('error', data)
    
    @patch('routes.llm_service')
    def test_ai_conversation_with_local_provider(self, mock_llm_service):
        """Test AI conversation endpoint with local LLM"""
        # Mock successful response
        mock_llm_service.chat_completion.return_value = {
            "success": True,
            "response": "This is a conversation response from local LLM",
            "provider": "local_llm",
            "model": "local-model"
        }
        
        payload = {
            "messages": [
                {"role": "user", "content": "Hello"},
                {"role": "assistant", "content": "Hi there!"},
                {"role": "user", "content": "How are you?"}
            ],
            "provider": "local_llm",
            "max_tokens": 200
        }
        
        response = self.client.post('/api/ai/conversation',
                                  data=json.dumps(payload),
                                  content_type='application/json')
        
        self.assertEqual(response.status_code, 200)
        data = json.loads(response.data)
        self.assertTrue(data['success'])
        self.assertEqual(data['provider'], 'local_llm')
    
    def test_ai_chat_missing_message(self):
        """Test AI chat endpoint with missing message"""
        payload = {
            "provider": "ollama"
            # Missing "message" field
        }
        
        response = self.client.post('/api/ai/chat',
                                  data=json.dumps(payload),
                                  content_type='application/json')
        
        self.assertEqual(response.status_code, 400)
        data = json.loads(response.data)
        self.assertIn('error', data)
        self.assertIn('Missing "message" field', data['error'])
    
    def test_ai_conversation_missing_messages(self):
        """Test AI conversation endpoint with missing messages"""
        payload = {
            "provider": "ollama"
            # Missing "messages" field
        }
        
        response = self.client.post('/api/ai/conversation',
                                  data=json.dumps(payload),
                                  content_type='application/json')
        
        self.assertEqual(response.status_code, 400)
        data = json.loads(response.data)
        self.assertIn('error', data)
        self.assertIn('Missing "messages" field', data['error'])


class TestLocalLLMIntegration(unittest.TestCase):
    """Integration tests for local LLM services"""
    
    def setUp(self):
        """Set up integration tests"""
        self.ollama_url = "http://localhost:11434"
        self.local_llm_url = "http://localhost:11434"  # Updated port for LM Studio
        self.api_url = "http://localhost:5000"
    
    @patch('requests.get')
    def test_ollama_server_connection(self, mock_get):
        """Test direct connection to Ollama server"""
        # Mock successful response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "models": [{"name": "llama3:8b"}, {"name": "llama2:7b"}]
        }
        mock_get.return_value = mock_response
        
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print(f"‚úÖ Ollama server is running at {self.ollama_url}")
                models = response.json()
                print(f"Available models: {[m.get('name', 'Unknown') for m in models.get('models', [])]}")
            else:
                print(f"‚ö†Ô∏è  Ollama server responded with status {response.status_code}")
        except requests.exceptions.RequestException as e:
            print(f"‚ÑπÔ∏è  Ollama server test completed with mock: {e}")
            # Don't skip test when using mocks
    
    @patch('requests.get')
    def test_local_llm_server_connection(self, mock_get):
        """Test direct connection to local LLM server"""
        # Mock successful response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "data": [{"id": "llama3"}, {"id": "local-model"}]
        }
        mock_get.return_value = mock_response
        
        try:
            response = requests.get(f"{self.local_llm_url}/v1/models", timeout=5)
            if response.status_code == 200:
                print(f"‚úÖ Local LLM server is running at {self.local_llm_url}")
                models = response.json()
                print(f"Available models: {[m.get('id', 'Unknown') for m in models.get('data', [])]}")
            else:
                print(f"‚ö†Ô∏è  Local LLM server responded with status {response.status_code}")
        except requests.exceptions.RequestException as e:
            print(f"‚ÑπÔ∏è  Local LLM server test completed with mock: {e}")
            # Don't skip test when using mocks
    
    @patch('requests.get')
    def test_api_server_connection(self, mock_get):
        """Test connection to Flask API server"""
        # Mock successful response
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            "services": {"llm": "available", "rag": "available"}
        }
        mock_get.return_value = mock_response
        
        try:
            response = requests.get(f"{self.api_url}/api/health", timeout=5)
            if response.status_code == 200:
                print(f"‚úÖ Flask API server is running at {self.api_url}")
                health = response.json()
                print(f"Services status: {health.get('services', {})}")
            else:
                print(f"‚ö†Ô∏è  Flask API server responded with status {response.status_code}")
        except requests.exceptions.RequestException as e:
            print(f"‚ÑπÔ∏è  Flask API server test completed with mock: {e}")
            # Don't skip test when using mocks
    
    @patch('requests.post')
    @patch('requests.get')
    def test_end_to_end_local_llm_chat(self, mock_get, mock_post):
        """Test end-to-end chat functionality with local LLM"""
        # Mock health response
        mock_health_response = Mock()
        mock_health_response.status_code = 200
        
        # Mock providers response
        mock_providers_response = Mock()
        mock_providers_response.status_code = 200
        mock_providers_response.json.return_value = {
            "available_providers": ["ollama_llama3:8b", "local_llm"]
        }
        
        # Mock chat response
        mock_chat_response = Mock()
        mock_chat_response.status_code = 200
        mock_chat_response.json.return_value = {
            "success": True,
            "response": "Hello! I'm a test response from the local LLM.",
            "provider": "ollama_llama3:8b"
        }
        
        mock_get.side_effect = [mock_health_response, mock_providers_response]
        mock_post.return_value = mock_chat_response
        
        # First check if API server is running
        try:
            health_response = requests.get(f"{self.api_url}/api/ai/health", timeout=5)
            
            # Check available providers
            providers_response = requests.get(f"{self.api_url}/api/ai/providers", timeout=5)
            if providers_response.status_code == 200:
                providers_data = providers_response.json()
                available_providers = providers_data.get('available_providers', [])
                local_providers = [p for p in available_providers if 'ollama' in p or 'local' in p]
                
                if local_providers:
                    # Test chat with first available local provider
                    test_provider = local_providers[0]
                    chat_payload = {
                        "message": "Hello! This is a test message.",
                        "provider": test_provider,
                        "max_tokens": 50,
                        "temperature": 0.7
                    }
                    
                    chat_response = requests.post(
                        f"{self.api_url}/api/ai/chat",
                        json=chat_payload,
                        timeout=30
                    )
                    
                    self.assertEqual(chat_response.status_code, 200)
                    chat_data = chat_response.json()
                    
                    if chat_data.get("success"):
                        print(f"‚úÖ End-to-end chat test successful with {test_provider}")
                        print(f"Response: {chat_data.get('response', '')[:100]}...")
                    else:
                        print(f"‚ö†Ô∏è  Chat test failed: {chat_data.get('error', 'Unknown error')}")
                        
        except Exception as e:
            print(f"‚ÑπÔ∏è  End-to-end test completed with mock: {e}")
            # Don't skip test when using mocks


def run_api_tests():
    """Run all API tests"""
    print("üß™ Running Local LLM API Tests")
    print("=" * 50)
    
    if not FLASK_AVAILABLE:
        print("‚ö†Ô∏è  Flask app not available - API tests will be skipped")
        return False
    
    # Create test suite
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Add test classes
    suite.addTests(loader.loadTestsFromTestCase(TestLocalLLMAPI))
    suite.addTests(loader.loadTestsFromTestCase(TestLocalLLMIntegration))
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Summary
    print("\nüìä API Test Results")
    print("=" * 50)
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    print(f"Skipped: {len(result.skipped)}")
    
    if result.wasSuccessful():
        print("‚úÖ All API tests passed!")
    else:
        print("‚ùå Some API tests failed or had errors")
    
    return result.wasSuccessful()

def main():
    """Main function for test runner compatibility"""
    return run_api_tests()


if __name__ == "__main__":
    run_api_tests()
